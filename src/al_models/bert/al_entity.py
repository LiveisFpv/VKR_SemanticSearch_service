import torch
import torch.nn.functional as F
from torch import Tensor
from transformers import AutoTokenizer, AutoModel
import os
import pandas as pd
import os
import tqdm
from src.lib.logger import Logger

class Model:
    def __init__(self, model_name, vector_dir, logger:Logger):
        self.__model_name = model_name
        self.__device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        logger.info(f"Starting {model_name}")
        self.__model = AutoModel.from_pretrained(self.__model_name).to(self.__device)
        self.__tokenizer = AutoTokenizer.from_pretrained(self.__model_name)
        
        logger.info(f"Started model {model_name}")
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï `.pth` —Ñ–∞–π–ª—ã –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.__vectors = self.__load_vectors(vector_dir)

    def __average_pool(self, last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:
        """–°—Ä–µ–¥–Ω–µ–µ –ø—É–ª–ª–∏–Ω–≥–æ–≤–∞–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π —Å —É—á–µ—Ç–æ–º –º–∞—Å–∫–∏."""
        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)
        return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]

    def __tokenize_text(self, text):
        """–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤."""
        inputs = self.__tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
        inputs = {k: v.to(self.__device) for k, v in inputs.items()}
        return inputs
    
    def __get_embedding(self, text):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞."""
        encoded_input = self.__tokenize_text(text)
        with torch.no_grad():
            model_output = self.__model(**encoded_input)
        
        embeddings = self.__average_pool(model_output.last_hidden_state, encoded_input["attention_mask"])
        embeddings = F.normalize(embeddings, p=2, dim=1)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        
        return embeddings.cpu()

    def __load_vectors(self, vector_dir):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏–∑ –í–°–ï–• `.pth` —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        all_vectors = []
        
        for file_name in os.listdir(vector_dir):
            if file_name.endswith(".pth"):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª - .pth
                file_path = os.path.join(vector_dir, file_name)
                loaded_vectors = torch.load(file_path)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞ –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫
                for item in loaded_vectors:
                    all_vectors.append({
                        "id": item["id"],
                        "vector": item["vector"],
                        "file_path": item["file_path"],  
                        "source_file": file_name  # –î–æ–±–∞–≤–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω –≤–µ–∫—Ç–æ—Ä
                    })

        return all_vectors
    
    def get_similar_texts(self, query_text, top_n=5):
        """–ü–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤."""
        query_embedding = self.__get_embedding(query_text)  # –≠–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = query_embedding.squeeze(0)  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarities = []
        for item in self.__vectors:
            vector = item["vector"]
            similarity = F.cosine_similarity(query_embedding, vector, dim=0)
            similarities.append((item["id"], item["file_path"], item["source_file"], similarity.item()))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarities.sort(key=lambda x: x[3], reverse=True)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º top_n –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö
        return similarities[:top_n]

    def add_paper(self, paper, data_path, vec_path):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–π –ø—É–±–ª–∏–∫–∞—Ü–∏–∏."""
        # TODO: —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–π –ø—É–±–ª–∏–∫–∞—Ü–∏–∏

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ csv —Ñ–∞–π–ª–æ–≤ –≤ pth

    def __load_articles_from_csv(self,file_path):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –∏–∑ CSV-—Ñ–∞–π–ª–∞."""
        articles = []
        df = pd.read_csv(file_path)
        
        required_columns = {"ID", "Title", "Abstract"}
        if not required_columns.issubset(df.columns):
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω —Ñ–∞–π–ª {file_path}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã")
            return []
        
        for _, row in df.iterrows():
            text = f"{row['Abstract']}"
            articles.append({
                "id": row["ID"],
                "text": text,
                "file_path": file_path
            })
        return articles

    def __tokenize_articles(self,articles, batch_size=16):
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–µ–π."""
        all_vectors = []
        
        for i in tqdm(range(0, len(articles), batch_size), desc="Vectorizing articles"):
            batch = articles[i:i+batch_size]
            texts = [article["text"] for article in batch]
            
            encoded_input = self.__tokenizer(texts, padding=True, truncation=True, return_tensors="pt", max_length=512)
            encoded_input = {k: v.to(self.__device) for k, v in encoded_input.items()}
            
            with torch.no_grad():
                model_output = model(**encoded_input)
            
            embeddings = self.__average_pool(model_output.last_hidden_state, encoded_input["attention_mask"])
            embeddings = F.normalize(embeddings, p=2, dim=1)
            
            for j, article in enumerate(batch):
                all_vectors.append({
                    "id": article["id"],
                    "vector": embeddings[j].cpu(),
                    "file_path": article["file_path"]
                })
        
        return all_vectors

    def __save_vectors_as_pth(vectors, output_file):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π PTH-—Ñ–∞–π–ª."""
        torch.save(vectors, output_file)

    def process_directory(self,dataset_path, output_path):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ CSV-—Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–∞ –æ—Ç–¥–µ–ª—å–Ω–æ."""
        os.makedirs(output_path, exist_ok=True)
        
        for file_name in os.listdir(dataset_path):
            if file_name.endswith('.csv'):
                file_path = os.path.join(dataset_path, file_name)
                output_file_pth = os.path.join(output_path, f"{os.path.splitext(file_name)[0]}.pth")
                
                articles = self.__load_articles_from_csv(file_path)
                if articles:
                    vectors = self.__tokenize_articles(articles)
                    self.__save_vectors_as_pth(vectors, output_file_pth)
                    print(f"‚úÖ –í–µ–∫—Ç–æ—Ä–∞ –¥–ª—è {file_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file_pth}")
                else:
                    print(f"‚ö†Ô∏è –ù–µ—Ç —Å—Ç–∞—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ {file_name}")
        

if __name__ == "__main__":
    vector_directory = "./data/vectorized/openAlex/"  # –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å `.pth` —Ñ–∞–π–ª–∞–º–∏
    model = Model("intfloat/multilingual-e5-large", vector_directory)

    query = "Machine learning for scientific research"
    top_matches = model.get_similar_texts(query, top_n=5)

    print("üîç –ù–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ —Ç–µ–∫—Å—Ç—ã:")
    for match in top_matches:
        print(f"üìÑ ID: {match[0]}, –§–∞–π–ª: {match[1]}, –ò—Å—Ç–æ—á–Ω–∏–∫: {match[2]}, –°—Ö–æ–¥—Å—Ç–≤–æ: {match[3]:.4f}")
